{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load necesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1      186      302       3  891717742\n",
       "3      244       51       2  880606923\n",
       "5      298      474       4  884182806\n",
       "6      115      265       2  881171488\n",
       "7      253      465       5  891628467"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ml-100k/u.data', names=['user_id', 'item_id', 'rating', 'timestamp'], sep='\\t')\n",
    "\n",
    "# obtain top 500 users and top 500 items\n",
    "user_ids = df.groupby('user_id').count().sort_values(by='rating', ascending=False).head(500).index\n",
    "item_ids = df.groupby('item_id').count().sort_values(by='rating', ascending=False).head(500).index\n",
    "df = df[(df['user_id'].isin(user_ids)) & (df['item_id'].isin(item_ids))]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select one rating from each user as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap user and item ID\n",
    "df['user_id'] = df.groupby('user_id').ngroup()\n",
    "df['item_id'] = df.groupby('item_id').ngroup()\n",
    "\n",
    "test_df = df.groupby('user_id').sample(1, random_state=1024)\n",
    "train_df = df[~df.index.isin(test_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of users: 500\n",
      "The number of items: 500\n",
      "Avg. # of rated Items/User: 129.914\n",
      "Density of data: 0.259828\n",
      "Ratings Range: 1 - 5\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "avg_num = df.groupby('user_id').size().mean()\n",
    "density = df.shape[0] / (n_users * n_items)\n",
    "min_ratings = df.rating.min()\n",
    "max_ratings = df.rating.max()\n",
    "\n",
    "print(\"The number of users: {}\" .format(n_users))\n",
    "print(\"The number of items: {}\" .format(n_items))\n",
    "print(\"Avg. # of rated Items/User: {}\" .format(avg_num))\n",
    "print(\"Density of data: {}\" .format(density))\n",
    "print(\"Ratings Range: {} - {}\" .format(min_ratings, max_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct the rating matrix based on train_df:\n",
      "[[5. 3. 4. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " [4. 3. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 4. 0.]]\n",
      "Construct the rating matrix based on test_df:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the format of datasets to matrices\n",
    "# Train dataset\n",
    "df_zeros = pd.DataFrame({\n",
    "    'user_id': np.tile(np.arange(0, n_users), n_items), \n",
    "    'item_id': np.repeat(np.arange(0, n_items), n_users), \n",
    "    'rating': 0})\n",
    "train_ds = df_zeros.merge(train_df, \n",
    "                          how='left', \n",
    "                          on=['user_id', 'item_id']).fillna(0.).pivot_table(\n",
    "                              values='rating_y', \n",
    "                              index='user_id', \n",
    "                              columns='item_id').values\n",
    "                           \n",
    "# Test dataset\n",
    "test_ds = df_zeros.merge(test_df, \n",
    "                         how='left', \n",
    "                         on=['user_id', 'item_id']).fillna(0.).pivot_table(\n",
    "                             values='rating_y', \n",
    "                             index='user_id', \n",
    "                             columns='item_id').values\n",
    "\n",
    "print(\"Construct the rating matrix based on train_df:\")\n",
    "print(train_ds)\n",
    "\n",
    "print(\"Construct the rating matrix based on test_df:\")\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-9\n",
    "\n",
    "def user_corr(imputed_train_ds):\n",
    "    '''\n",
    "    Function for calculating user's similarity\n",
    "    '''\n",
    "    active_user_pearson_corr = np.zeros((imputed_train_ds.shape[0], imputed_train_ds.shape[0]))\n",
    "\n",
    "    # Compute Pearson Correlation Coefficient of All Pairs of Users between active set and training dataset\n",
    "    for i, user_i_vec in enumerate(imputed_train_ds):\n",
    "        for j, user_j_vec in enumerate(imputed_train_ds):\n",
    "\n",
    "            # ratings corated by the current pair od users\n",
    "            mask_i = user_i_vec > 0\n",
    "            mask_j = user_j_vec > 0\n",
    "\n",
    "            # corrated item index, skip if there are no corrated ratings\n",
    "            corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "            if len(corrated_index) == 0:\n",
    "                continue\n",
    "\n",
    "            # average value of user_i_vec and user_j_vec\n",
    "            mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "            mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "            # compute pearson corr\n",
    "            user_i_sub_mean = user_i_vec[corrated_index] - mean_user_i\n",
    "            user_j_sub_mean = user_j_vec[corrated_index] - mean_user_j\n",
    "\n",
    "            r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "            r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "            r_ui_sum_sqrt = np.sqrt(np.sum(r_ui_sub_r_i_sq))\n",
    "            r_uj_sum_sqrt = np.sqrt(np.sum(r_uj_sub_r_j_sq))\n",
    "\n",
    "            sim = np.sum(user_i_sub_mean * user_j_sub_mean) / (r_ui_sum_sqrt * r_uj_sum_sqrt + EPSILON)\n",
    "            active_user_pearson_corr[i][j] = sim\n",
    "\n",
    "    return active_user_pearson_corr\n",
    "\n",
    "def predict(test_ds, imputed_train_ds, user_corr, k=20):\n",
    "    '''\n",
    "    Function for predicting ratings in test_ds\n",
    "    '''\n",
    "\n",
    "    # Predicting ratings of test set\n",
    "    predicted_ds = np.zeros_like(test_ds)\n",
    "\n",
    "    for (i, j), rating in np.ndenumerate(test_ds):\n",
    "\n",
    "        if rating > 0:\n",
    "\n",
    "            # only predict ratings on test set\n",
    "            sim_user_ids = np.argsort(user_corr[i])[-1:-(k + 1):-1]\n",
    "\n",
    "            #==================user-based==================#\n",
    "            # the coefficient values of similar users\n",
    "            sim_val = user_corr[i][sim_user_ids]\n",
    "\n",
    "            # the average value of the current user's ratings\n",
    "            sim_users = imputed_train_ds[sim_user_ids]\n",
    "            \n",
    "            mask_rateditem_user = imputed_train_ds[i] != 0\n",
    "            num_rated_items = mask_rateditem_user.astype(np.float32)\n",
    "            user_mean = np.sum(imputed_train_ds[i, mask_rateditem_user]) / (num_rated_items.sum() + EPSILON)\n",
    "\n",
    "            mask_nei_rated_items = sim_users != 0\n",
    "            num_rated_per_user = mask_nei_rated_items.astype(np.float32)\n",
    "            num_per_user = num_rated_per_user.sum(axis=1)\n",
    "\n",
    "            sum_per_user = sim_users.sum(axis=1)\n",
    "            sim_user_mean = sum_per_user / (num_per_user + EPSILON)\n",
    "            \n",
    "            mask_rated_j = sim_users[:, j] > 0\n",
    "                            \n",
    "            # sim(u, v) * (r_vj - mean_v)\n",
    "            sim_r_sum_mean = sim_val[mask_rated_j] * (sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])\n",
    "            \n",
    "            user_based_pred = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "\n",
    "            predicted_ds[i, j] = np.clip(user_based_pred, 0, 5)\n",
    "            \n",
    "    return predicted_ds\n",
    "\n",
    "def evaluate(test_ds, predicted_ds):\n",
    "    '''\n",
    "    Function for evaluating on MAE and RMSE\n",
    "    '''\n",
    "    # MAE\n",
    "    mask_test_ds = test_ds > 0\n",
    "    MAE = np.sum(np.abs(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32))\n",
    "\n",
    "    # RMSE\n",
    "    RMSE = np.sqrt(np.sum(np.square(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32)))\n",
    "\n",
    "    return MAE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - KNN based recommendation (Similarity Metric: Pearson Correlation Coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pearson_corr = user_corr(train_ds)\n",
    "predicted_ds = predict(test_ds, train_ds, user_pearson_corr, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Baseline Result =====================\n",
      "MAE: 0.8471711011333851, RMSE: 1.092846045041526\n"
     ]
    }
   ],
   "source": [
    "MAE, RMSE = evaluate(test_ds, predicted_ds)\n",
    "\n",
    "print(\"===================== Baseline Result =====================\")\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user-KNN based recommendation (Similarity Metric: Pearson Correlation Coefficient utilising item popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of Top 5 recommended items for active user 2 are: [499 170 157 158 159]\n",
      "The index of Top 5 recommended items for a new user are: [ 36 128 135  77  75]\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 1e-9\n",
    "\n",
    "def user_corr2(imputed_train_ds):\n",
    "    '''\n",
    "    Function for calculating user's similarity utilising item popularity\n",
    "    '''\n",
    "    active_user_pearson_corr = np.zeros((imputed_train_ds.shape[0], imputed_train_ds.shape[0]))\n",
    "    # Step A: Compute item popularity for every items\n",
    "    popularity_vec = np.count_nonzero(imputed_train_ds, axis=0)\n",
    "    \n",
    "    # Step B: Compute Pearson Correlation Coefficient of All Pairs of Users between active sets in training dataset\n",
    "    \n",
    "    # make a copy of the training dataset\n",
    "    train_cur = imputed_train_ds.copy()\n",
    "    \n",
    "    # for each pairs of active users\n",
    "    for i, user_i_vec in enumerate(train_cur):\n",
    "        for j, user_j_vec in enumerate(train_cur):\n",
    "\n",
    "            # a) Rating data making up\n",
    "            # a) Step 1: find the union set of items voted by user i or user j\n",
    "            i_rated = np.where(user_i_vec > 0)\n",
    "            j_rated = np.where(user_j_vec > 0)\n",
    "            rated_index = np.union1d(i_rated, j_rated)\n",
    "            \n",
    "            \n",
    "            # a) Step 2: fill missing values of the union set with the average rating of corresponding user\n",
    "             \n",
    "                # average rating of user_i_vec and user_j_vec\n",
    "            mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "            mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "           \n",
    "                # find the set of items voted by user i but not voted by user j and vice versa\n",
    "            j_notrated = np.setdiff1d(i_rated,j_rated)\n",
    "            i_notrated = np.setdiff1d(j_rated,i_rated)\n",
    "\n",
    "                # fill missing values with the average rating of corresponding user\n",
    "            user_i_vec[i_notrated] = mean_user_i\n",
    "            user_j_vec[j_notrated] = mean_user_j\n",
    "            \n",
    "            # corrated item index, skip if there are no corrated ratings\n",
    "            if len(rated_index) == 0:\n",
    "                continue\n",
    "                        \n",
    "            \n",
    "            # b) Compute similarity utilising item popularity\n",
    "            \n",
    "                # popularity significance weight of item t\n",
    "            popularity_t = popularity_vec[rated_index]     \n",
    "            weight_t = np.log(imputed_train_ds.shape[0]/(popularity_t + EPSILON))\n",
    "            weight_t_sq = np.square(weight_t)\n",
    "           \n",
    "\n",
    "                # compute pearson correlation coefficient with item popularity weight\n",
    "            user_i_sub_mean = user_i_vec[rated_index] - mean_user_i\n",
    "            user_j_sub_mean = user_j_vec[rated_index] - mean_user_j\n",
    "\n",
    "            r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "            r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "\n",
    "            r_w_ui_sum_sqrt = np.sqrt(np.sum(weight_t_sq*r_ui_sub_r_i_sq))\n",
    "            r_w_uj_sum_sqrt = np.sqrt(np.sum(weight_t_sq*r_uj_sub_r_j_sq))\n",
    "\n",
    "            sim = np.sum(weight_t_sq*user_i_sub_mean * user_j_sub_mean) / (r_w_ui_sum_sqrt * r_w_uj_sum_sqrt + EPSILON)\n",
    "            active_user_pearson_corr[i][j] = sim\n",
    "\n",
    "    return active_user_pearson_corr\n",
    "\n",
    "def predict2(test_ds, imputed_train_ds, user_corr, k=20):\n",
    "    '''\n",
    "    Function for predicting ratings in test_ds\n",
    "    '''\n",
    "\n",
    "    # Predicting ratings of test set\n",
    "    predictions = np.zeros_like(test_ds)\n",
    "\n",
    "    for (i, j), rating in np.ndenumerate(test_ds):\n",
    "\n",
    "        if rating > 0:\n",
    "\n",
    "            # only predict ratings on test set, \n",
    "            # find top-k most similar users as the current user, remove itself\n",
    "            sim_user_ids = np.argsort(user_corr[i])[-1:-(k + 1):-1]\n",
    "\n",
    "            #==================user-based==================#\n",
    "            # the coefficient values of similar users\n",
    "            sim_val = user_corr[i][sim_user_ids]\n",
    "\n",
    "            # the average value of the current user's ratings\n",
    "            sim_users = imputed_train_ds[sim_user_ids]\n",
    "            user_mean = np.sum(imputed_train_ds[i]) / (np.sum(np.clip(imputed_train_ds[i], 0, 1)) + EPSILON)\n",
    "            sim_user_mean = np.sum(sim_users, axis=1) / (np.sum(np.clip(sim_users, 0, 1), axis=1) + EPSILON)\n",
    "\n",
    "            # select the users who rated item j           \n",
    "            mask_rated_j = sim_users[:, j] > 0\n",
    "                            \n",
    "            # sim(x, u) * (r_uj - mean_u)\n",
    "            sim_r_sum_mean = sim_val[mask_rated_j] * (sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])\n",
    "            \n",
    "            user_based_pred = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "\n",
    "            predictions[i, j] = np.clip(user_based_pred, 0, 5)\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "def evaluate2(test_ds, predictions):\n",
    "    '''\n",
    "    Function for evaluating on MAE and RMSE\n",
    "    '''\n",
    "    # MAE (Compute mean absolute error)\n",
    "    mask_test_ds = test_ds > 0\n",
    "    MAE = np.sum(np.abs(test_ds[mask_test_ds] - predictions[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32))\n",
    "\n",
    "    # RMSE (Compute root mean square error)\n",
    "    RMSE = np.sqrt(np.sum(np.square(test_ds[mask_test_ds] - predictions[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32)))\n",
    "\n",
    "    return MAE, RMSE\n",
    "\n",
    "\n",
    "MAE = 0 # 0 is an intial value\n",
    "RMSE = 0 # 0 is an intial value\n",
    "\n",
    "\n",
    "user_pearson_corr = user_corr2(train_ds)\n",
    "predictions = predict2(test_ds, train_ds, user_pearson_corr, k=20)\n",
    "MAE, RMSE = evaluate2(test_ds, predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Recommendation list of top_N items\n",
    "\n",
    "def recommend_active(prediction, dataforpredict, top_N=5, activeuser = 2):\n",
    "    '''\n",
    "    Function for generating the recommendation items list for active user a \n",
    "    based on the Top-N highest prediction value.\n",
    "    '''\n",
    "    # Make a copy of prediction set\n",
    "    pred_cur = prediction.copy()\n",
    "    \n",
    "    # If the active user i rated item j already, we exclude item j from prediction/recommendations\n",
    "    # by setting its value to zero\n",
    "    for (i, j), rating in np.ndenumerate(dataforpredict):\n",
    "        if rating >0:\n",
    "            pred_cur[i][j] = 0\n",
    "            \n",
    "    # Make top_N recommendations to the active user by sorting the predictions of that users for all items        \n",
    "    itemSortInd = pred_cur[activeuser,:].argsort()[::-1][:top_N]\n",
    "    \n",
    "    reclist = print('The index of Top', top_N, 'recommended items for active user', activeuser, 'are:', itemSortInd)\n",
    "    \n",
    "    return reclist\n",
    "\n",
    "\n",
    "# the recommendation items list for active user 2 based on the Top-5 highest prediction value.\n",
    "rec_active = recommend_active(predictions, test_ds, top_N=5, activeuser = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comRV(dataset, top_N=5):\n",
    "    '''\n",
    "    Function for listing the items with the Top-N highest composite recommendation values for a new user.\n",
    "    '''\n",
    "    EPSILON = 1e-9\n",
    "    \n",
    "    # Compute the item popularity for every items\n",
    "    popularity_vec = np.count_nonzero(dataset, axis=0)\n",
    "    \n",
    "    # Compute the sum of rating for every items\n",
    "    itemRateSum = dataset.sum(axis=0)\n",
    "    \n",
    "    # Compute the average of rating for every items\n",
    "    mean_r_t = itemRateSum/popularity_vec\n",
    "   \n",
    "    \n",
    "    # Compute composite recommendation value for every item\n",
    "    comRV_t = popularity_vec*(mean_r_t + EPSILON)\n",
    "    \n",
    "    # Sort the comRV of all items and get the indexes of top_N items with the highest values\n",
    "    itemSortInd = comRV_t.argsort()[::-1][:top_N]\n",
    "    \n",
    "    # List the Top-N most popular items and which are also rated well.\n",
    "    reclist = print('The index of Top', top_N, 'recommended items for a new user are:', itemSortInd)\n",
    "    \n",
    "    return reclist\n",
    "\n",
    "# recommendation list of top_5 items for new user \n",
    "# utilising composite recommendation values from training set\n",
    "rec_new = comRV(train_ds, top_N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the MAE and RMSE of New Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== The MAE and RMSE of Your Implementation =====================\n",
      "MAE: 0.7655375639836793, RMSE: 0.9874731529656616\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "print(\"===================== The MAE and RMSE of Your Implementation =====================\")\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
